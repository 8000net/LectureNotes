{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforce Learning Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States and Actions\n",
    "\n",
    "![alt text](images/states_actions.png)\n",
    "\n",
    "The first core concept we will cover is the understanding of what states and actions are. Reinforcement learning is a type of machine learning that is agent-oriented; it relies on its enviroment rather than a teacher to achieve its desired goal. This is similar to how humans learn, through the steps of trial and error.\n",
    "\n",
    "Let's take for example a person learning to navigate a maze. A state can compropise of any crossroad they are met with, an action is defined as a choice/direction they choose to go, and the goal (reward) is defined as them reaching the end of the maze.\n",
    "\n",
    "As the person navigates the maze, they will naturally discover that some paths are less optimal than others, while some do not ever reach the end. Ideally, over time, they would be able to navigate the most optimal path every time. And this is what we are trying to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process \n",
    "\n",
    "Building on top of states and actions is the next step, a Markov Decision Process (MDP). A MDP can be simplified to a tuple containing 5 parts:\n",
    "   \n",
    "S - set of states   \n",
    "A - set of actions   \n",
    "P - probability that an action *a* at state *s* at time *t* will get to state *s + 1* at time *t + 1*   \n",
    "R - reward received after moving from state *s* to state *s + 1*   \n",
    "$\\gamma$ - discount factor that can optimize future rewards vs present rewards\n",
    "   \n",
    "Each of these play a role in determining a final \"policy\" $\\pi$; a rule that says given a state *s*, action *a* will be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "This brings us to building our first algorithm, Q-Learning. Let's go over the formula:   \n",
    "   \n",
    "$$Q({s_t, a_t}) \\mathrel{+}= Q({s_t, a_t}) + \\alpha[r_{t+1} +\\gamma*\\max_a Q(s_{s_t}, a) - Q(s_t, a_t)]$$\n",
    "   \n",
    "$\\alpha$ - the learning rate, typically a small value between 0 and 1, indicates how much we update over values every time we take an action. Typically this value tends to be smaller in order not to overrepresent certain action. \n",
    "    \n",
    "$\\gamma$ - discount factor, encourages an agent to seek a reward sooner than later, typically set to .9~.99, makes agents receive a smaller reward in the present to give better incentive for future rewards\n",
    "\n",
    "Given this formula, you need the apply it using the following steps:\n",
    "1. Set initial value of *Q(s, a)* to all abritary values.   \n",
    "2. Eventually while reaching the limit, make sure to do all actions *a* for all states *s*.\n",
    "3. At each time *t*, change one element.\n",
    "4. You could reduce the $\\alpha$ element over time for optimization purposes.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gridworld Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Image #1: http://slideplayer.com/slide/2342696/\n",
    "\n",
    "Youtube: https://youtu.be/ggqnxyjaKe4\n",
    "\n",
    "Youtube: https://youtu.be/3T5eCou2erg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
